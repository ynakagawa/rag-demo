# ðŸ Vercel Python Runtime Deployment Guide

## Overview

Your chatbot now uses **Vercel's Python runtime** (Beta) to deploy the LangChain backend as serverless functions. This is the best solution because:

âœ… Keep your Python code with full LangChain support  
âœ… Native LangSmith tracing integration  
âœ… Use all Python AI libraries  
âœ… Serverless scaling on Vercel  
âœ… No need to convert to Node.js  

Reference: [Vercel Python Runtime Documentation](https://vercel.com/docs/functions/runtimes/python)

## ðŸ“ Project Structure

```
RAG-demo/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ chat.py          # Python serverless function for chat
â”‚   â”œâ”€â”€ reset.py         # Python serverless function for reset
â”‚   â””â”€â”€ health.py        # Python serverless function for health
â”œâ”€â”€ server.js            # Node.js Express for frontend UI
â”œâ”€â”€ views/
â”‚   â””â”€â”€ chat.ejs        # Chat interface
â”œâ”€â”€ requirements.txt     # Python dependencies for Vercel
â”œâ”€â”€ vercel.json         # Vercel configuration
â””â”€â”€ .env                # Environment variables (not deployed)
```

## ðŸ—ï¸ Architecture

### On Vercel (Production)
```
Browser â†’ Node.js (Express) â†’ Python Serverless Functions â†’ OpenAI
                                      â†“
                              LangSmith Tracing
```

### Local Development
```
Browser â†’ Node.js (Express) â†’ Python Flask (agent_api.py) â†’ OpenAI
                                      â†“
                              LangSmith Tracing
```

## ðŸš€ Deploy to Vercel

### Step 1: Push to GitHub

Your code is already on GitHub at: `https://github.com/ynakagawa/rag-demo`

### Step 2: Import to Vercel

1. Go to [vercel.com](https://vercel.com)
2. Click "Add New Project"
3. Import your GitHub repository: `ynakagawa/rag-demo`
4. Vercel will automatically detect:
   - Python runtime for `api/*.py` files
   - Node.js runtime for `server.js`
   - Dependencies from `requirements.txt` and `package.json`

### Step 3: Configure Environment Variables

In your Vercel project settings, add these environment variables:

**Required:**
- `OPENAI_API_KEY` = Your OpenAI API key

**Optional (for LangSmith tracing):**
- `LANGSMITH_API_KEY` = Your LangSmith API key
- `LANGSMITH_TRACING` = `true`
- `LANGSMITH_PROJECT` = Your project name (e.g., `pr-pertinent-bookend-77`)
- `LANGSMITH_ENDPOINT` = `https://api.smith.langchain.com`

### Step 4: Deploy

Click "Deploy" and Vercel will:
1. Install Python dependencies from `requirements.txt`
2. Install Node.js dependencies from `package.json`
3. Build your Python functions
4. Build your Node.js frontend
5. Deploy everything as serverless functions

You'll get a URL like: `https://rag-demo.vercel.app`

## ðŸ”§ Python Runtime Details

### Python Version
- **Python 3.12** (cannot be changed)

### Dependencies
- Listed in `requirements.txt`
- Automatically installed by Vercel
- Include all LangChain packages

### Bundle Size Limit
- Maximum **250 MB** uncompressed
- Current setup is well under this limit

### Supported Frameworks
- âœ… Flask (what we're using)
- âœ… FastAPI
- âœ… Django
- âœ… WSGI applications
- âœ… ASGI applications

## ðŸ’» Local Development

You can still develop locally using either:

**Option 1: Original Flask server (Recommended)**
```bash
# Terminal 1 - Python Backend
cd /Users/ynaka/Documents/RAG-demo
source venv/bin/activate
python agent_api.py

# Terminal 2 - Node.js Frontend
node server.js
```

**Option 2: Vercel CLI (Test production-like environment)**
```bash
# Install Vercel CLI
npm install -g vercel

# Run locally
vercel dev
```

**Option 3: Use the startup script**
```bash
./start.sh
```

## ðŸ“Š LangSmith Tracing

LangSmith tracing works seamlessly with Python serverless functions:

1. Set environment variables in Vercel dashboard
2. All LangChain calls are automatically traced
3. View traces at: https://smith.langchain.com
4. Filter by project name

## âš™ï¸ Configuration Files

### `requirements.txt`
Lists all Python dependencies:
- Flask for HTTP handling
- LangChain for AI orchestration
- OpenAI for LLM access
- LangSmith for tracing

### `vercel.json`
Configures Vercel deployment:
- Uses `@vercel/python` for Python files
- Uses `@vercel/node` for Node.js files
- Routes API calls to Python functions
- Routes UI requests to Express server

## ðŸ”„ Differences from Node.js Approach

| Feature | Python Runtime | Node.js Runtime |
|---------|---------------|-----------------|
| LangChain Support | âœ… Full support | âš ï¸ Limited |
| LangSmith Integration | âœ… Native | âš ï¸ Manual |
| Code Reuse | âœ… Same as local | âŒ Need rewrite |
| AI Libraries | âœ… All Python libs | âš ï¸ JS only |
| Setup Complexity | âœ… Simple | âš ï¸ More complex |

## ðŸ› Troubleshooting

### Issue: Python function timeout
**Solution:** Vercel has a 10-second timeout on Hobby plan (60s on Pro)
- Optimize your prompts
- Use streaming responses
- Upgrade to Pro plan if needed

### Issue: Import errors
**Solution:** Make sure all dependencies are in `requirements.txt`
```bash
pip freeze > requirements.txt
```

### Issue: Environment variables not working
**Solution:** 
- Add them in Vercel dashboard (not in vercel.json)
- Redeploy after adding variables
- Check exact variable names

### Issue: Cold starts
**Solution:** This is normal for serverless
- First request may be slow (~2-5 seconds)
- Subsequent requests are fast
- Consider keeping functions warm with cron jobs

### Issue: 250 MB bundle size exceeded
**Solution:** Exclude unnecessary files
```json
{
  "functions": {
    "api/*.py": {
      "excludeFiles": "tests/**"
    }
  }
}
```

## ðŸŽ¯ Production Recommendations

### 1. Add a Database for Conversation Persistence
Currently using in-memory storage (resets on each function call).

**Recommended options:**
- **Vercel KV** (Redis) - Best for Vercel
- **Upstash Redis** - Serverless Redis
- **MongoDB Atlas** - Document database
- **Supabase** - PostgreSQL

### 2. Implement Rate Limiting
```python
from flask_limiter import Limiter

limiter = Limiter(
    app,
    key_func=lambda: request.headers.get('X-Forwarded-For', request.remote_addr)
)

@app.route('/api/chat', methods=['POST'])
@limiter.limit("10 per minute")
def chat():
    # ... your code
```

### 3. Add Authentication
- Use Vercel's built-in authentication
- Or implement your own with JWT
- Protect sensitive endpoints

### 4. Enable Streaming
```python
from flask import Response, stream_with_context

@app.route('/api/chat', methods=['POST'])
def chat():
    def generate():
        for chunk in llm.stream(messages):
            yield f"data: {chunk.content}\n\n"
    
    return Response(
        stream_with_context(generate()),
        mimetype='text/event-stream'
    )
```

### 5. Monitor and Optimize
- Use Vercel Analytics
- Monitor LangSmith traces
- Set up error tracking (Sentry)
- Track costs and usage

## ðŸ“š Resources

- [Vercel Python Runtime Documentation](https://vercel.com/docs/functions/runtimes/python)
- [Vercel Functions](https://vercel.com/docs/functions)
- [Flask Documentation](https://flask.palletsprojects.com/)
- [LangChain Python Documentation](https://python.langchain.com/)
- [LangSmith Documentation](https://docs.smith.langchain.com/)

## âœ¨ Next Steps

1. âœ… Deploy to Vercel with Python runtime
2. ðŸ”„ Add database for conversation persistence
3. ðŸ”’ Implement authentication
4. ðŸ“Š Set up monitoring and analytics
5. ðŸš€ Add RAG with vector stores (Pinecone, Weaviate)
6. âš¡ Implement streaming responses
7. ðŸŽ¨ Customize the UI

## ðŸ†š Why This is Better

**Before (Node.js API functions):**
- Had to rewrite Python code in JavaScript
- Limited LangChain features
- Manual LangSmith integration
- Two different codebases

**Now (Python runtime):**
- âœ… Same Python code everywhere
- âœ… Full LangChain ecosystem
- âœ… Native LangSmith support
- âœ… One codebase, works everywhere

Your deployment is now production-ready with the full power of Python and LangChain! ðŸŽ‰

